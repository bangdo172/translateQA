{"qas": [{"question": "What are the latest generation of lossless algorithms? ", "id": "57268bcdf1498d1400e8e35a", "answers": [{"text": "Genetics", "answer_start": 0}], "is_impossible": false}, {"question": "What encoding reduces the heterogeneity of a dataset by sorting SNPs?", "id": "57268bcdf1498d1400e8e35b", "answers": [{"text": "MAFE", "answer_start": 693}], "is_impossible": false}, {"question": "What two algorithms have compression ratios of up to 1200-fold?", "id": "57268bcdf1498d1400e8e35c", "answers": [{"text": "DNAZip and GenomeZip", "answer_start": 862}], "is_impossible": false}, {"plausible_answers": [{"text": "Genetics", "answer_start": 0}], "question": "What are the latest generation of DNAZip and GEnomeZip?", "id": "5a66c596f038b7001ab0c108", "answers": [], "is_impossible": true}, {"plausible_answers": [{"text": "MAFE", "answer_start": 693}], "question": "What encoding reduces the heterogenelty of a dataset by sorting megabytes?", "id": "5a66c596f038b7001ab0c109", "answers": [], "is_impossible": true}, {"plausible_answers": [{"text": "DNAZip and GenomeZip", "answer_start": 862}], "question": "What two algorithms have allele frequency of up to 1200-fold?", "id": "5a66c596f038b7001ab0c10a", "answers": [], "is_impossible": true}, {"plausible_answers": [{"text": "HAPZIPPER", "answer_start": 397}], "question": "What provides 2- to 4- fold better heterogenelty than the leading gemeral-purpose compression utilities?", "id": "5a66c596f038b7001ab0c10b", "answers": [], "is_impossible": true}, {"plausible_answers": [{"text": "Johns Hopkins University", "answer_start": 276}], "question": "Who published a genetic compression algorithm that does not use diploids?", "id": "5a66c596f038b7001ab0c10c", "answers": [], "is_impossible": true}], "context": "<a0_0><b3_0>Genetics<b3_0/><a0_0/> compression algorithms are the latest generation of lossless algorithms that compress data (typically sequences of nucleotides) using both conventional compression algorithms and genetic algorithms adapted to the specific datatype. In 2012, a team of scientists from <b7_0>Johns Hopkins University<b7_0/> published a genetic compression algorithm that does not use a reference genome for compression. <b6_0>HAPZIPPER<b6_0/> was tailored for HapMap data and achieves over 20-fold compression (95% reduction in file size), providing 2- to 4-fold better compression and in much faster time than the leading general-purpose compression utilities. For this, Chanda, Elhaik, and Bader introduced MAF based encoding (<a1_0><b4_0>MAFE<b4_0/><a1_0/>), which reduces the heterogeneity of the dataset by sorting SNPs by their minor allele frequency, thus homogenizing the dataset. Other algorithms in 2009 and 2013 (<a2_0><b5_0>DNAZip and GenomeZip<b5_0/><a2_0/>) have compression ratios of up to 1200-fold\u2014allowing 6 billion basepair diploid human genomes to be stored in 2.5 megabytes (relative to a reference genome or averaged over many genomes)."}