{"qas": [{"question": "Which judge presided over the Netbula v. Chordiant case?", "id": "56ddc6e266d3e219004daceb", "answers": [{"text": "Magistrate Judge Howard Lloyd", "answer_start": 0}], "is_impossible": false}, {"question": "In what jurisdiction was the Netbula v. Chordiant case tried?", "id": "56ddc6e266d3e219004dacec", "answers": [{"text": "Northern District of California, San Jose Division", "answer_start": 37}], "is_impossible": false}, {"question": "Which party won its argument regarding Netbula's robots.txt file?", "id": "56ddc6e266d3e219004daced", "answers": [{"text": "Chordiant", "answer_start": 200}], "is_impossible": false}, {"plausible_answers": [{"text": "Magistrate Judge Howard Lloyd", "answer_start": 0}], "question": "Which judge presided over the California v. Chordiant case?", "id": "5a6b1401a9e0c9001a4e9ec4", "answers": [], "is_impossible": true}, {"plausible_answers": [{"text": "Northern District of California, San Jose", "answer_start": 37}], "question": "In what jurisdiction was the California v. Chordiant case tried?", "id": "5a6b1401a9e0c9001a4e9ec5", "answers": [], "is_impossible": true}, {"plausible_answers": [{"text": "Chordiant", "answer_start": 200}], "question": "Which party won its argument regarding California's robots.txt file?", "id": "5a6b1401a9e0c9001a4e9ec6", "answers": [], "is_impossible": true}, {"plausible_answers": [{"text": "Magistrate Judge Howard Lloyd", "answer_start": 0}], "question": "Who rejected California's arguments?", "id": "5a6b1401a9e0c9001a4e9ec7", "answers": [], "is_impossible": true}, {"plausible_answers": [{"text": "to allow Chordiant to retrieve the archived pages that they sought", "answer_start": 191}], "question": "Why was California allowed to disable the robots.txt blockage?", "id": "5a6b1401a9e0c9001a4e9ec8", "answers": [], "is_impossible": true}], "context": "<a0_0><b3_0><b6_0>Magistrate Judge Howard Lloyd<b6_0/><b3_0/><a0_0/> in the <a1_0><b4_0>Northern District of California, San Jose<b4_0/> Division<a1_0/>, rejected Netbula's arguments and ordered them to disable the robots.txt blockage temporarily in order <b7_0>to allow <a2_0><b5_0>Chordiant<b5_0/><a2_0/> to retrieve the archived pages that they sought<b7_0/>."}