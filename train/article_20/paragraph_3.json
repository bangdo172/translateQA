{"qas": [{"question": "When was the program necessary to crawl and archive the web created?", "id": "56ddbae166d3e219004dacaf", "answers": [{"text": "1996", "answer_start": 3}], "is_impossible": false}, {"question": "What is a term used for programs that automatically visit websites and record the data they find?", "id": "56ddbae166d3e219004dacb0", "answers": [{"text": "crawlers", "answer_start": 260}], "is_impossible": false}, {"question": "What rule do crawlers abide by when determining which sites to record?", "id": "56ddbae166d3e219004dacb1", "answers": [{"text": "robots exclusion standard", "answer_start": 470}], "is_impossible": false}, {"question": "What site was created to mitigate issues with incomplete copies of websites?", "id": "56ddbae166d3e219004dacb2", "answers": [{"text": "Archive-It.org", "answer_start": 640}], "is_impossible": false}, {"plausible_answers": [{"text": "1996", "answer_start": 3}], "question": "When was the program necessary to include all information available on the interet created?", "id": "5a6b01dba9e0c9001a4e9e56", "answers": [], "is_impossible": true}, {"plausible_answers": [{"text": "crawlers", "answer_start": 260}], "question": "What is a term used for programs that automatically visit websites and record the board system they find?", "id": "5a6b01dba9e0c9001a4e9e57", "answers": [], "is_impossible": true}, {"plausible_answers": [{"text": "robots exclusion standard", "answer_start": 470}], "question": "What rule do web sites abide by when determining sites to record?", "id": "5a6b01dba9e0c9001a4e9e58", "answers": [], "is_impossible": true}, {"plausible_answers": [{"text": "Archive-It.org", "answer_start": 640}], "question": "What site was created to mitigate issues with Bruce Gilliat?", "id": "5a6b01dba9e0c9001a4e9e59", "answers": [], "is_impossible": true}, {"plausible_answers": [{"text": "1996", "answer_start": 3}], "question": "When did Brewster Kate and Bruce Gilliat develop software to download crawlwers?", "id": "5a6b01dba9e0c9001a4e9e5a", "answers": [], "is_impossible": true}], "context": "In <a0_0><b4_0><b8_0>1996<b8_0/><b4_0/><a0_0/> Brewster Kahle, with Bruce Gilliat, developed software to crawl and download all publicly accessible World Wide Web pages, the Gopher hierarchy, the Netnews (Usenet) bulletin board system, and downloadable software. The information collected by these \"<a1_0><b5_0>crawlers<b5_0/><a1_0/>\" does not include all the information available on the Internet, since much of the data is restricted by the publisher or stored in databases that are not accessible. These \"crawlers\" also respect the <a2_0><b6_0>robots exclusion standard<b6_0/><a2_0/> for websites whose owners opt for them not to appear in search results or be cached. To overcome inconsistencies in partially cached web sites, <a3_0><b7_0>Archive-It.org<b7_0/><a3_0/> was developed in 2005 by the Internet Archive as a means of allowing institutions and content creators to voluntarily harvest and preserve collections of digital content, and create digital archives."}